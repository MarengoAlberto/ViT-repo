{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import storage\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "APP_NAME = 'vit'\n",
    "MODEL_PT_FILEPATH = 'saved_models/VisionTransformers'\n",
    "MAR_MODEL_OUT_PATH = 'serve'\n",
    "handler = 'predictor/handler.py'\n",
    "MODEL_DISPLAY_NAME = 'ViT-model'\n",
    "model_version = 1\n",
    "PROJECT_ID = 'alberto-playground'\n",
    "BUCKET_NAME = 'alberto-vit-playground'\n",
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Running archiver command: torch-model-archiver --force --model-name ViT-model --serialized-file saved_models/VisionTransformers/ViT.pt --handler predictor/handler.py --version 1 --export-path serve/model-store --extra-files src/model/__pycache__,src/model/modules.py,src/model/__init__.py,src/model/.ipynb_checkpoints,src/model/model.py\n"
     ]
    }
   ],
   "source": [
    "# # create directory to save model archive file\n",
    "# model_output_root = MODEL_PT_FILEPATH\n",
    "# mar_output_root = MAR_MODEL_OUT_PATH\n",
    "# additiona_files_base_dir = 'src/model'\n",
    "# export_path = f\"{mar_output_root}/model-store\"\n",
    "# try:\n",
    "#     Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "# except Exception as e:\n",
    "#     logging.warning(e)\n",
    "#     # retry after pause\n",
    "#     time.sleep(2)\n",
    "#     Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "#\n",
    "# # parse and configure paths for model archive config\n",
    "# handler_path = (\n",
    "#     handler.replace(\"gs://\", \"/gcs/\") + \"predictor/handler.py\"\n",
    "#     if handler.startswith(\"gs://\")\n",
    "#     else handler\n",
    "# )\n",
    "# model_artifacts_dir = model_output_root\n",
    "# extra_files = [\n",
    "#     os.path.join(additiona_files_base_dir, f)\n",
    "#     for f in os.listdir(additiona_files_base_dir)]\n",
    "#\n",
    "# # define model archive config\n",
    "# mar_config = {\n",
    "#     \"MODEL_NAME\": MODEL_DISPLAY_NAME,\n",
    "#     \"HANDLER\": handler_path,\n",
    "#     \"SERIALIZED_FILE\": f'{model_artifacts_dir}/ViT.pt',\n",
    "#     \"VERSION\": model_version,\n",
    "#     \"EXTRA_FILES\": \",\".join(extra_files),\n",
    "#     \"EXPORT_PATH\": export_path,\n",
    "# }\n",
    "#\n",
    "# # generate model archive command\n",
    "# archiver_cmd = (\n",
    "#     \"torch-model-archiver --force \"\n",
    "#     f\"--model-name {mar_config['MODEL_NAME']} \"\n",
    "#     f\"--serialized-file {mar_config['SERIALIZED_FILE']} \"\n",
    "#     f\"--handler {mar_config['HANDLER']} \"\n",
    "#     f\"--version {mar_config['VERSION']}\"\n",
    "# )\n",
    "# if \"EXPORT_PATH\" in mar_config:\n",
    "#     archiver_cmd += f\" --export-path {mar_config['EXPORT_PATH']}\"\n",
    "# if \"EXTRA_FILES\" in mar_config:\n",
    "#     archiver_cmd += f\" --extra-files {mar_config['EXTRA_FILES']}\"\n",
    "# if \"REQUIREMENTS_FILE\" in mar_config:\n",
    "#     archiver_cmd += f\" --requirements-file {mar_config['REQUIREMENTS_FILE']}\"\n",
    "#\n",
    "# # run archiver command\n",
    "# logging.warning(\"Running archiver command: %s\", archiver_cmd)\n",
    "# with subprocess.Popen(\n",
    "#         archiver_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "# ) as p:\n",
    "#     _, err = p.communicate()\n",
    "#     if err:\n",
    "#         raise ValueError(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage.Client().bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f'{MAR_MODEL_OUT_PATH}/ViT-model.mar')\n",
    "blob.upload_from_filename('serve/model-store/ViT-model.mar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  394.7MB\n",
      "Step 1/21 : FROM pytorch/torchserve:latest-cpu\n",
      " ---> 68a3fcae81af\n",
      "Step 2/21 : USER root\n",
      " ---> Using cache\n",
      " ---> 74b7dbf2b479\n",
      "Step 3/21 : RUN apt-get update &&     apt-get install -y software-properties-common &&     add-apt-repository -y ppa:ubuntu-toolchain-r/test &&     apt-get update &&     apt-get install -y gcc-9 g++-9 apt-transport-https ca-certificates gnupg curl\n",
      " ---> Using cache\n",
      " ---> 6e360930db3d\n",
      "Step 4/21 : RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" |     tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg |     apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - &&     apt-get update -y &&     apt-get install google-cloud-sdk -y\n",
      " ---> Using cache\n",
      " ---> bfe0359200e4\n",
      "Step 5/21 : USER model-server\n",
      " ---> Using cache\n",
      " ---> 2d2bde191019\n",
      "Step 6/21 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 37c9ec098c4f\n",
      "Step 7/21 : COPY serve/model-store/ViT-model.mar /home/model-server/model-store/\n",
      " ---> a7bf560abf08\n",
      "Step 8/21 : ARG MODEL_NAME=ViT-model\n",
      " ---> Running in 16b5f1631e6b\n",
      "Removing intermediate container 16b5f1631e6b\n",
      " ---> dd16bb391920\n",
      "Step 9/21 : ENV MODEL_NAME=\"${MODEL_NAME}\"\n",
      " ---> Running in 4f44c7372fc1\n",
      "Removing intermediate container 4f44c7372fc1\n",
      " ---> b88816484800\n",
      "Step 10/21 : ARG MAR_URI=gs://alberto-playground/serve\n",
      " ---> Running in 572f9bd0d59b\n",
      "Removing intermediate container 572f9bd0d59b\n",
      " ---> 09007f467c19\n",
      "Step 11/21 : ENV MAR_URI=\"${AIP_STORAGE_URI}\"\n",
      " ---> Running in 48f5e7d8e6eb\n",
      "Removing intermediate container 48f5e7d8e6eb\n",
      " ---> 5f415b7c251a\n",
      "Step 12/21 : ARG AIP_HTTP_PORT=7080\n",
      " ---> Running in 47f7c6cd558c\n",
      "Removing intermediate container 47f7c6cd558c\n",
      " ---> df2ec151f841\n",
      "Step 13/21 : ENV AIP_HTTP_PORT=\"${AIP_HTTP_PORT}\"\n",
      " ---> Running in 5afe07f55c41\n",
      "Removing intermediate container 5afe07f55c41\n",
      " ---> e43a86db7e27\n",
      "Step 14/21 : ARG MODEL_MGMT_PORT=7081\n",
      " ---> Running in 416b488ebb10\n",
      "Removing intermediate container 416b488ebb10\n",
      " ---> 2244608bbe35\n",
      "Step 15/21 : EXPOSE \"${AIP_HTTP_PORT}\"\n",
      " ---> Running in e762bfec9d3c\n",
      "Removing intermediate container e762bfec9d3c\n",
      " ---> 8da4127b4d56\n",
      "Step 16/21 : EXPOSE \"${MODEL_MGMT_PORT}\"\n",
      " ---> Running in 0b7a374b481b\n",
      "Removing intermediate container 0b7a374b481b\n",
      " ---> 7fa71d382d34\n",
      "Step 17/21 : EXPOSE 8080 8081 8082 7070 7071\n",
      " ---> Running in 8fc8649b5a88\n",
      "Removing intermediate container 8fc8649b5a88\n",
      " ---> efa7758a6b69\n",
      "Step 18/21 : USER root\n",
      " ---> Running in db958be1391f\n",
      "Removing intermediate container db958be1391f\n",
      " ---> f8962727ad33\n",
      "Step 19/21 : RUN echo \"service_envelope=json\\n\"     \"inference_address=http://0.0.0.0:${AIP_HTTP_PORT}\\n\"     \"management_address=http://0.0.0.0:${MODEL_MGMT_PORT}\" >>     /home/model-server/config.properties\n",
      " ---> Running in 33bb695b490e\n",
      "Removing intermediate container 33bb695b490e\n",
      " ---> 2724a245085a\n",
      "Step 20/21 : USER model-server\n",
      " ---> Running in fd808e6589b4\n",
      "Removing intermediate container fd808e6589b4\n",
      " ---> b4858e13af43\n",
      "Step 21/21 : CMD [\"echo\", \"MAR_URI=${MAR_URI}\", \";\",    \"ls\", \"-ltr\", \"/home/model-server/model-store/\", \";\",     \"torchserve\", \"--start\", \"--ts-config=/home/model-server/config.properties\",     \"--models\", \"${MODEL_NAME}=${MODEL_NAME}.mar\",     \"--model-store\", \"/home/model-server/model-store\"]\n",
      " ---> Running in 25ff2d307abe\n",
      "Removing intermediate container 25ff2d307abe\n",
      " ---> d828cd8693b5\n",
      "Successfully built d828cd8693b5\n",
      "Successfully tagged gcr.io/alberto-playground/pytorch_predict_vit:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -f predictor/Dockerfile -t $CUSTOM_PREDICTOR_IMAGE_URI ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/alberto-playground/pytorch_predict_vit]\n",
      "\n",
      "\u001B[1Bcb124e95: Preparing \n",
      "\u001B[1Bc1975f18: Preparing \n",
      "\u001B[1Be1b71f7f: Preparing \n",
      "\u001B[1B7b334d17: Preparing \n",
      "\u001B[1B001bafce: Preparing \n",
      "\u001B[1Bbf18a086: Preparing \n",
      "\u001B[1B7cf25f52: Preparing \n",
      "\u001B[1Bfa8107fa: Preparing \n",
      "\u001B[1B24bd1a34: Preparing \n",
      "\u001B[1B0b544b4c: Preparing \n",
      "\u001B[1B613e1d99: Preparing \n",
      "\u001B[1Bb3c8b2c4: Preparing \n",
      "\u001B[1B0ae33361: Preparing \n",
      "\u001B[13B1975f18: Pushed   11.84MB/11.84MB\u001B[10A\u001B[2K\u001B[8A\u001B[2K\u001B[7A\u001B[2K\u001B[4A\u001B[2K\u001B[1A\u001B[2K\u001B[13A\u001B[2K\u001B[13A\u001B[2K\u001B[13A\u001B[2K\u001B[13A\u001B[2K\u001B[13A\u001B[2K\u001B[14A\u001B[2K\u001B[13A\u001B[2Klatest: digest: sha256:0c84648e648f80fd0f6dbcc4cb99b33403160df541dc9b9dff89b22df1a033e4 size: 3253\n"
     ]
    }
   ],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_display_name = f\"{APP_NAME}-v{model_version}\"\n",
    "model_description = \"PyTorch Image classifier with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model backing LRO: projects/634066980332/locations/us-central1/models/8835704028110258176/operations/8659418435475734528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/634066980332/locations/us-central1/models/8835704028110258176/operations/8659418435475734528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Resource name: projects/634066980332/locations/us-central1/models/8835704028110258176@1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/634066980332/locations/us-central1/models/8835704028110258176@1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this Model in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = aiplatform.Model('projects/634066980332/locations/us-central1/models/8835704028110258176@1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/634066980332/locations/us-central1/models/8835704028110258176@1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit-v1\n",
      "projects/634066980332/locations/us-central1/models/8835704028110258176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    "    artifact_uri=f'gs://{BUCKET_NAME}/{MAR_MODEL_OUT_PATH}',\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Endpoint backing LRO: projects/634066980332/locations/us-central1/endpoints/1493361090891874304/operations/3205559286730063872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/634066980332/locations/us-central1/endpoints/1493361090891874304/operations/3205559286730063872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created. Resource name: projects/634066980332/locations/us-central1/endpoints/1493361090891874304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/634066980332/locations/us-central1/endpoints/1493361090891874304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this Endpoint in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint = aiplatform.Endpoint('projects/634066980332/locations/us-central1/endpoints/1493361090891874304')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/634066980332/locations/us-central1/endpoints/1493361090891874304')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/634066980332/locations/us-central1/endpoints/1493361090891874304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/634066980332/locations/us-central1/endpoints/1493361090891874304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy Endpoint model backing LRO: projects/634066980332/locations/us-central1/endpoints/1493361090891874304/operations/9199850390760194048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/634066980332/locations/us-central1/endpoints/1493361090891874304/operations/9199850390760194048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint model deployed. Resource name: projects/634066980332/locations/us-central1/endpoints/1493361090891874304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/634066980332/locations/us-central1/endpoints/1493361090891874304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7fa3a92c19c0> \n",
       "resource name: projects/634066980332/locations/us-central1/endpoints/1493361090891874304"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name = vit-endpoint resource id =projects/634066980332/locations/us-central1/endpoints/1493361090891874304 \n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"3764008732900458496\"\n",
       " model: \"projects/634066980332/locations/us-central1/models/8835704028110258176\"\n",
       " display_name: \"vit-v1\"\n",
       " create_time {\n",
       "   seconds: 1690256597\n",
       "   nanos: 595458000\n",
       " }\n",
       " dedicated_resources {\n",
       "   machine_spec {\n",
       "     machine_type: \"n1-standard-4\"\n",
       "   }\n",
       "   min_replica_count: 1\n",
       "   max_replica_count: 1\n",
       " }\n",
       " model_version_id: \"1\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_images = 'gs://alberto-vit-playground/samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = [{\"input\": test_images}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailable",
     "evalue": "503 Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001B[0m, in \u001B[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallable_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:946\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    944\u001B[0m state, call, \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(request, timeout, metadata, credentials,\n\u001B[1;32m    945\u001B[0m                               wait_for_ready, compression)\n\u001B[0;32m--> 946\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_end_unary_response_blocking\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcall\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:849\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m    848\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 849\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
      "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.1.95:443 {created_time:\"2023-07-25T04:05:09.734800197+00:00\", grpc_status:14, grpc_message:\"Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mServiceUnavailable\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstances\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_instance\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1559\u001B[0m, in \u001B[0;36mEndpoint.predict\u001B[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001B[0m\n\u001B[1;32m   1546\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Prediction(\n\u001B[1;32m   1547\u001B[0m         predictions\u001B[38;5;241m=\u001B[39mjson_response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1548\u001B[0m         deployed_model_id\u001B[38;5;241m=\u001B[39mraw_predict_response\u001B[38;5;241m.\u001B[39mheaders[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1556\u001B[0m         ),\n\u001B[1;32m   1557\u001B[0m     )\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1559\u001B[0m     prediction_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prediction_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gca_resource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1561\u001B[0m \u001B[43m        \u001B[49m\u001B[43minstances\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minstances\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1563\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1566\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Prediction(\n\u001B[1;32m   1567\u001B[0m         predictions\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m   1568\u001B[0m             json_format\u001B[38;5;241m.\u001B[39mMessageToDict(item)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1573\u001B[0m         model_resource_name\u001B[38;5;241m=\u001B[39mprediction_response\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[1;32m   1574\u001B[0m     )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001B[0m, in \u001B[0;36mPredictionServiceClient.predict\u001B[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001B[0m\n\u001B[1;32m    597\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(metadata) \u001B[38;5;241m+\u001B[39m (\n\u001B[1;32m    598\u001B[0m     gapic_v1\u001B[38;5;241m.\u001B[39mrouting_header\u001B[38;5;241m.\u001B[39mto_grpc_metadata(((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mendpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m, request\u001B[38;5;241m.\u001B[39mendpoint),)),\n\u001B[1;32m    599\u001B[0m )\n\u001B[1;32m    601\u001B[0m \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[0;32m--> 602\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrpc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;66;03m# Done; return the response.\u001B[39;00m\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001B[0m, in \u001B[0;36m_GapicCallable.__call__\u001B[0;34m(self, timeout, retry, *args, **kwargs)\u001B[0m\n\u001B[1;32m    110\u001B[0m     metadata\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata)\n\u001B[1;32m    111\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m metadata\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001B[0m, in \u001B[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m callable_(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mfrom_grpc_error(exc) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[0;31mServiceUnavailable\u001B[0m: 503 Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496"
     ]
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances=test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instances: Union[Dict, List[Dict]],\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    \"\"\"\n",
    "    `instances` can be either single instance of type dict or a list\n",
    "    of instances.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances = instances if type(instances) == list else [instances]\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "    ]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\", dict(prediction))\n",
    "\n",
    "\n",
    "# [END aiplatform_predict_custom_trained_model_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailable",
     "evalue": "503 Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001B[0m, in \u001B[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallable_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:946\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    944\u001B[0m state, call, \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(request, timeout, metadata, credentials,\n\u001B[1;32m    945\u001B[0m                               wait_for_ready, compression)\n\u001B[0;32m--> 946\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_end_unary_response_blocking\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcall\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:849\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m    848\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 849\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
      "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:173.194.195.95:443 {created_time:\"2023-07-25T04:07:21.571434852+00:00\", grpc_status:14, grpc_message:\"Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mServiceUnavailable\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpredict_custom_trained_model_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproject\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m634066980332\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mendpoint_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m1493361090891874304\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mus-central1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43minstances\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minstance_key_1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 34\u001B[0m, in \u001B[0;36mpredict_custom_trained_model_sample\u001B[0;34m(project, endpoint_id, instances, location, api_endpoint)\u001B[0m\n\u001B[1;32m     30\u001B[0m parameters \u001B[38;5;241m=\u001B[39m json_format\u001B[38;5;241m.\u001B[39mParseDict(parameters_dict, Value())\n\u001B[1;32m     31\u001B[0m endpoint \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mendpoint_path(\n\u001B[1;32m     32\u001B[0m     project\u001B[38;5;241m=\u001B[39mproject, location\u001B[38;5;241m=\u001B[39mlocation, endpoint\u001B[38;5;241m=\u001B[39mendpoint_id\n\u001B[1;32m     33\u001B[0m )\n\u001B[0;32m---> 34\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minstances\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minstances\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m deployed_model_id:\u001B[39m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mdeployed_model_id)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001B[0m, in \u001B[0;36mPredictionServiceClient.predict\u001B[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001B[0m\n\u001B[1;32m    597\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(metadata) \u001B[38;5;241m+\u001B[39m (\n\u001B[1;32m    598\u001B[0m     gapic_v1\u001B[38;5;241m.\u001B[39mrouting_header\u001B[38;5;241m.\u001B[39mto_grpc_metadata(((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mendpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m, request\u001B[38;5;241m.\u001B[39mendpoint),)),\n\u001B[1;32m    599\u001B[0m )\n\u001B[1;32m    601\u001B[0m \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[0;32m--> 602\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrpc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;66;03m# Done; return the response.\u001B[39;00m\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001B[0m, in \u001B[0;36m_GapicCallable.__call__\u001B[0;34m(self, timeout, retry, *args, **kwargs)\u001B[0m\n\u001B[1;32m    110\u001B[0m     metadata\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata)\n\u001B[1;32m    111\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m metadata\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001B[0m, in \u001B[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m callable_(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m grpc\u001B[38;5;241m.\u001B[39mRpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mfrom_grpc_error(exc) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[0;31mServiceUnavailable\u001B[0m: 503 Model server is not available. Please retry. endpoint_id: 1493361090891874304, deployed_model_id: 3764008732900458496"
     ]
    }
   ],
   "source": [
    "predict_custom_trained_model_sample(\n",
    "    project=\"634066980332\",\n",
    "    endpoint_id=\"1493361090891874304\",\n",
    "    location=\"us-central1\",\n",
    "    instances={ \"instance_key_1\": \"value\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
